/**
 * ONE-TIME RESET FUNCTION (CORRECTED AND FULLY SAFE)
 * Reads all data from a specified source spreadsheet and completely OVERWRITES the
 * web app's main CSV files.
 * THIS VERSION IS NOW SAFE FOR ALL FILES: It uses master header lists to guarantee
 * that the column order and names in all destination CSVs are never changed.
 */
function resetAndImportFromSourceSheet() {
  // --- CONFIGURATION: VERIFY THESE ARE CORRECT ---
  const SOURCE_SPREADSHEET_ID = "1ZQUcYK81yYYwRjuyG99agU4bWipfTehe5AXXRZEC2SM";
  const SOURCE_TASK_SHEET_NAME = 'Sub Task Sheet';
  const SOURCE_ESCALATION_SHEET_NAME = 'Escalation Logs';
  const SOURCE_PAUSE_SHEET_NAME = 'Pause Logs';

  // --- MASTER HEADER LISTS FOR CSV FILES ---
  // Note: For best practice, these constants should eventually be moved to code.gs alongside SUB_TASK_HEADERS.
  const ESCALATION_LOGS_HEADERS = [
    'Log ID', 'Related Case ID', 'Market', 'Task Type',
    'Escalation Start Time', 'Escalation End Time'
  ];
  const PAUSE_LOGS_HEADERS = [
    'Log ID', 'Related Case ID', 'Market', 'Task type',
    'Pause Start Time', 'Pause End Time'
  ];
  // ---------------------------------------------

  Logger.log('Starting data reset and import process...');

  const lock = LockService.getScriptLock();
  lock.waitLock(30000);

  try {
    const sourceSpreadsheet = SpreadsheetApp.openById(SOURCE_SPREADSHEET_ID);

    // --- 1. Process Sub Tasks ---
    const taskSheet = sourceSpreadsheet.getSheetByName(SOURCE_TASK_SHEET_NAME);
    if (taskSheet) {
      const taskData = taskSheet.getDataRange().getValues();
      if (taskData.length > 1) {
        const taskHeaders = taskData.shift();
        const taskObjects = taskData.map(row => {
          let obj = {};
          taskHeaders.forEach((header, index) => {
            obj[header.trim()] = row[index];
          });
          return obj;
        });

        // Normalize the object keys to match the master header list
        const normalizedTaskObjects = taskObjects.map(obj => {
          const newObj = {};
          const headerMap = { 'Task Type': 'Task type' }; // Keep this for compatibility

          for (const key in obj) {
            const newKey = headerMap[key] || key;
            newObj[newKey] = obj[key];
          }
          return newObj;
        });

        Logger.log(`Found ${normalizedTaskObjects.length} task records in the source sheet. Overwriting destination CSV...`);

        // This is the SAFE write operation using the master header list from code.gs
        writeCSV(SUB_TASK_FILE_ID, normalizedTaskObjects, SUB_TASK_HEADERS);

        Logger.log('Sub Task CSV has been reset and overwritten successfully and safely.');

      } else {
        Logger.log(`Source sheet '${SOURCE_TASK_SHEET_NAME}' is empty. Writing empty file.`);
        // Also use master headers here to write an empty file with the correct header row
        writeCSV(SUB_TASK_FILE_ID, [], SUB_TASK_HEADERS);
      }
    } else {
      Logger.log(`ERROR: Source sheet named '${SOURCE_TASK_SHEET_NAME}' was not found. Skipping.`);
    }

    // --- 2. Process Escalation Logs (NOW SAFE) ---
    const escalationSheet = sourceSpreadsheet.getSheetByName(SOURCE_ESCALATION_SHEET_NAME);
    if (escalationSheet) {
      const escalationData = escalationSheet.getDataRange().getValues();
      if (escalationData.length > 1) {
        const escalationHeaders = escalationData.shift();
        const escalationObjects = escalationData.map(row => {
            let obj = {};
            escalationHeaders.forEach((header, index) => { obj[header.trim()] = row[index]; });
            return obj;
        });

        // Add a normalization step for consistency, although the headers should match
        const normalizedEscalationObjects = escalationObjects.map(obj => {
          const newObj = {};
          const headerMap = { 'Task type': 'Task Type' }; // Standardize to capital 'T'
          for (const key in obj) {
            const newKey = headerMap[key] || key;
            newObj[newKey] = obj[key];
          }
          return newObj;
        });

        Logger.log(`Found ${normalizedEscalationObjects.length} escalation records. Overwriting...`);
        // Use the new master header list to ensure a safe write
        writeCSV(ESCALATION_LOGS_FILE_ID, normalizedEscalationObjects, ESCALATION_LOGS_HEADERS);
        Logger.log('Escalation Logs CSV has been reset safely.');
      } else {
        Logger.log(`Source sheet '${SOURCE_ESCALATION_SHEET_NAME}' is empty. Writing empty file.`);
        writeCSV(ESCALATION_LOGS_FILE_ID, [], ESCALATION_LOGS_HEADERS);
      }
    } else {
      Logger.log(`ERROR: Source sheet named '${SOURCE_ESCALATION_SHEET_NAME}' was not found.`);
    }

    // --- 3. Process Pause Logs (NOW SAFE) ---
    const pauseSheet = sourceSpreadsheet.getSheetByName(SOURCE_PAUSE_SHEET_NAME);
    if (pauseSheet) {
      const pauseData = pauseSheet.getDataRange().getValues();
      if (pauseData.length > 1) {
        const pauseHeaders = pauseData.shift();
        const pauseObjects = pauseData.map(row => {
            let obj = {};
            pauseHeaders.forEach((header, index) => { obj[header.trim()] = row[index]; });
            return obj;
        });

        // Add a normalization step for consistency
        const normalizedPauseObjects = pauseObjects.map(obj => {
          const newObj = {};
          const headerMap = { 'Task Type': 'Task type' }; // Standardize to lowercase 't'
          for (const key in obj) {
            const newKey = headerMap[key] || key;
            newObj[newKey] = obj[key];
          }
          return newObj;
        });

        Logger.log(`Found ${normalizedPauseObjects.length} pause records. Overwriting...`);
        // Use the new master header list to ensure a safe write
        writeCSV(PAUSE_LOGS_FILE_ID, normalizedPauseObjects, PAUSE_LOGS_HEADERS);
        Logger.log('Pause Logs CSV has been reset safely.');
      } else {
        Logger.log(`Source sheet '${SOURCE_PAUSE_SHEET_NAME}' is empty. Writing empty file.`);
        writeCSV(PAUSE_LOGS_FILE_ID, [], PAUSE_LOGS_HEADERS);
      }
    } else {
      Logger.log(`ERROR: Source sheet named '${SOURCE_PAUSE_SHEET_NAME}' was not found.`);
    }

    Logger.log('Data reset process completed successfully!');

  } catch (e) {
    Logger.log(`A critical error occurred during the reset process: ${e.message}`);
  } finally {
    lock.releaseLock();
  }
}




// =================================================================
// ===== NEW EXPORT FUNCTIONS (SPLIT TO AVOID TIMEOUTS) =========
// =================================================================

/**
 * Helper function to process a single CSV file and export it to a new Google Sheet.
 * This is not meant to be run directly.
 */
function _exportSingleCsvToSheet(fileName, fileId, blueprintHeaders) {
  Logger.log(`Starting export process for ${fileName}...`);
  try {
    const timestamp = Utilities.formatDate(new Date(), Session.getScriptTimeZone(), "yyyy-MM-dd HH:mm");
    const reportSpreadsheet = SpreadsheetApp.create(`${fileName} Export - ${timestamp}`);
    const reportUrl = reportSpreadsheet.getUrl();
    Logger.log(`Report for ${fileName} created. URL: ${reportUrl}`);

    const sheet = reportSpreadsheet.getSheets()[0].setName(fileName);
    sheet.getRange(1, 1, 1, blueprintHeaders.length).setValues([blueprintHeaders]).setFontWeight('bold');
    sheet.setFrozenRows(1);

    const dataArray = (function(fileId) {
      try {
        const file = DriveApp.getFileById(fileId);
        const content = file.getBlob().getDataAsString();
        if (!content || content.trim() === '') return [];
        return Utilities.parseCsv(content);
      } catch(e) {
        Logger.log(`Failed to read CSV ${fileId}. Error: ${e.message}`);
        return [];
      }
    })(fileId);

    if (!dataArray || dataArray.length <= 1) {
      Logger.log(`No data rows to write for ${fileName}.`);
      return;
    }

    const csvHeaders = dataArray.shift();
    const headerMap = blueprintHeaders.map(h => csvHeaders.indexOf(h));

    const batchSize = 500;
    Logger.log(`Writing ${dataArray.length} records to ${sheet.getName()} in batches...`);

    for (let i = 0; i < dataArray.length; i += batchSize) {
      const chunk = dataArray.slice(i, i + batchSize);
      const orderedBatch = chunk.map(row => headerMap.map(index => (index === -1) ? '' : row[index]));
      if (orderedBatch.length > 0) {
        sheet.getRange(i + 2, 1, orderedBatch.length, blueprintHeaders.length).setValues(orderedBatch);
        Logger.log(`Wrote batch of ${orderedBatch.length} rows.`);
        SpreadsheetApp.flush();
      }
    }

    sheet.autoResizeColumns(1, blueprintHeaders.length);
    Logger.log(`Successfully finished writing all records for ${fileName}.`);

  } catch(e) {
    Logger.log(`A critical error occurred during the export of ${fileName}: ${e.message}`);
  }
}

/**
 * Exports the Sub Task CSV data to a new Google Sheet.
 * Run this function from the Apps Script editor.
 */
function exportSubTasksCsvToSheet() {
  const blueprintHeaders = [
    'Log ID', 'Date', 'Case ID', 'Market', 'User', 'Task type', 'Start Timestamp', 'End Timestamp',
    'OBQ Reasons', 'Case Created', 'Status', 'Comments', 'Escalation Reasons', 'Stored Escalation Duration',
    'Stored Paused Duration', 'Asset Trigger Date', 'Asset Delivery Date', 'OnBoarding Done',
    'Flag Asset Action', 'Flag OnBoarding Done', 'Post OBQ Tags', 'isReadyToDelete'
  ];
  _exportSingleCsvToSheet('Sub Task', SUB_TASK_FILE_ID, blueprintHeaders);
}

/**
 * Exports the Escalation Logs CSV data to a new Google Sheet.
 * Run this function from the Apps Script editor.
 */
function exportEscalationsCsvToSheet() {
  const blueprintHeaders = [
    'Log ID', 'Related Case ID', 'Market', 'Task Type',
    'Escalation Start Time', 'Escalation End Time', 'Escalation Reasons', 'OBQ_SLA_Status'
  ];
  _exportSingleCsvToSheet('Escalation Logs', ESCALATION_LOGS_FILE_ID, blueprintHeaders);
}

/**
 * Exports the Pause Logs CSV data to a new Google Sheet.
 * Run this function from the Apps Script editor.
 */
function exportPausesCsvToSheet() {
  const blueprintHeaders = [
    'Log ID', 'Related Case ID', 'Market', 'Task type',
    'Pause Start Time', 'Pause End Time'
  ];
  _exportSingleCsvToSheet('Pause Logs', PAUSE_LOGS_FILE_ID, blueprintHeaders);
}










// =================================================================
// ===== NEW EXPORT FUNCTIONS (SPLIT TO AVOID TIMEOUTS) =========
// =================================================================

/**
 * Helper function to process a single CSV file and export it to a Google Sheet.
 * Can create a new spreadsheet or write to an existing one.
 * This is not meant to be run directly.
 */
function _exportSingleCsvToSheet(fileName, fileId, blueprintHeaders, targetSpreadsheetId = null) {
  Logger.log(`Starting export process for ${fileName}...`);
  try {
    let reportSpreadsheet;
    let sheet;

    if (targetSpreadsheetId) {
      // Logic for writing to an EXISTING spreadsheet
      reportSpreadsheet = SpreadsheetApp.openById(targetSpreadsheetId);
      Logger.log(`Opened existing spreadsheet for ${fileName}. URL: ${reportSpreadsheet.getUrl()}`);
      sheet = reportSpreadsheet.getSheetByName(fileName);
      if (sheet) {
        // Overwrite by clearing the sheet completely
        sheet.clear();
        Logger.log(`Cleared existing sheet: ${fileName}`);
      } else {
        // If sheet doesn't exist, create it
        sheet = reportSpreadsheet.insertSheet(fileName);
        Logger.log(`Created new sheet: ${fileName}`);
      }
    } else {
      // Logic for creating a NEW spreadsheet
      const timestamp = Utilities.formatDate(new Date(), Session.getScriptTimeZone(), "yyyy-MM-dd HH:mm");
      reportSpreadsheet = SpreadsheetApp.create(`${fileName} Export - ${timestamp}`);
      const reportUrl = reportSpreadsheet.getUrl();
      Logger.log(`Report for ${fileName} created. URL: ${reportUrl}`);
      sheet = reportSpreadsheet.getSheets()[0].setName(fileName);
    }

    // --- Common logic for writing data ---

    sheet.getRange(1, 1, 1, blueprintHeaders.length).setValues([blueprintHeaders]).setFontWeight('bold');
    sheet.setFrozenRows(1);

    const dataArray = (function(fileId) {
      try {
        const file = DriveApp.getFileById(fileId);
        const content = file.getBlob().getDataAsString();
        if (!content || content.trim() === '') return [];
        return Utilities.parseCsv(content);
      } catch(e) {
        Logger.log(`Failed to read CSV ${fileId}. Error: ${e.message}`);
        return [];
      }
    })(fileId);

    if (!dataArray || dataArray.length <= 1) {
      Logger.log(`No data rows to write for ${fileName}.`);
      return;
    }

    const csvHeaders = dataArray.shift();
    const headerMap = blueprintHeaders.map(h => csvHeaders.indexOf(h));

    const batchSize = 500;
    Logger.log(`Writing ${dataArray.length} records to ${sheet.getName()} in batches...`);

    for (let i = 0; i < dataArray.length; i += batchSize) {
      const chunk = dataArray.slice(i, i + batchSize);
      const orderedBatch = chunk.map(row => headerMap.map(index => (index === -1) ? '' : row[index]));
      if (orderedBatch.length > 0) {
        sheet.getRange(i + 2, 1, orderedBatch.length, blueprintHeaders.length).setValues(orderedBatch);
        Logger.log(`Wrote batch of ${orderedBatch.length} rows.`);
        SpreadsheetApp.flush();
      }
    }

    sheet.autoResizeColumns(1, blueprintHeaders.length);
    Logger.log(`Successfully finished writing all records for ${fileName}.`);

  } catch(e) {
    Logger.log(`A critical error occurred during the export of ${fileName}: ${e.message}`);
  }
}

/**
 * Exports the Sub Task CSV data to a specific, existing Google Sheet.
 */
function exportSubTasksCsvToSheet() {
  const blueprintHeaders = [
    'Log ID', 'Date', 'Case ID', 'Market', 'User', 'Task type', 'Start Timestamp', 'End Timestamp',
    'OBQ Reasons', 'Case Created', 'Status', 'Comments', 'Escalation Reasons', 'Stored Escalation Duration',
    'Stored Paused Duration', 'Asset Trigger Date', 'Asset Delivery Date', 'OnBoarding Done',
    'Flag Asset Action', 'Flag OnBoarding Done', 'Post OBQ Tags', 'isReadyToDelete'
  ];
  // THE FIX: Added the targetSpreadsheetId to this function
  const targetSpreadsheetId = '1ztZmrSgZ6sQ8CcXC0pPqmwgbfbjW2u9GumliZbNXwsg';
  _exportSingleCsvToSheet('Sub Task', SUB_TASK_FILE_ID, blueprintHeaders, targetSpreadsheetId);
}

/**
 * Exports the Escalation Logs CSV data to a specific, existing Google Sheet.
 */
function exportEscalationsCsvToSheet() {
  const blueprintHeaders = [
    'Log ID', 'Related Case ID', 'Market', 'Task Type',
    'Escalation Start Time', 'Escalation End Time', 'Escalation Reasons', 'OBQ_SLA_Status'
  ];
  const targetSpreadsheetId = '1ztZmrSgZ6sQ8CcXC0pPqmwgbfbjW2u9GumliZbNXwsg';
  _exportSingleCsvToSheet('Escalation Logs', ESCALATION_LOGS_FILE_ID, blueprintHeaders, targetSpreadsheetId);
}

/**
 * Exports the Pause Logs CSV data to a specific, existing Google Sheet.
 */
function exportPausesCsvToSheet() {
  const blueprintHeaders = [
    'Log ID', 'Related Case ID', 'Market', 'Task type',
    'Pause Start Time', 'Pause End Time'
  ];
  const targetSpreadsheetId = '1ztZmrSgZ6sQ8CcXC0pPqmwgbfbjW2u9GumliZbNXwsg';
  _exportSingleCsvToSheet('Pause Logs', PAUSE_LOGS_FILE_ID, blueprintHeaders, targetSpreadsheetId);
}

// =================================================================
// ===== NEW SEQUENTIAL EXPORT FUNCTIONS (TO AVOID TIMEOUTS) =======
// =================================================================

/**
 * STARTER FUNCTION: Manually run this function to begin the sequential export process.
 * It cleans up old triggers and starts the first export task.
 */
function runAllExportsInSequence() {
  // Clean up any old triggers that might be left over from previous runs
  const allTriggers = ScriptApp.getProjectTriggers();
  for (const trigger of allTriggers) {
    if (trigger.getHandlerFunction() === '_continueExportSequence') {
      ScriptApp.deleteTrigger(trigger);
    }
  }

  Logger.log('Starting sequential export...');
  // Set a property to mark which step we are on.
  PropertiesService.getScriptProperties().setProperty('exportStep', '1');

  // Create a trigger to run the next part of the sequence almost immediately.
  ScriptApp.newTrigger('_continueExportSequence')
      .timeBased()
      .after(10 * 1000) // 10 seconds
      .create();

  Logger.log('First export task has been scheduled. Check execution logs for progress.');
}

/**
 * WORKER FUNCTION: This function is called by a trigger and should not be run manually.
 * It reads the current step, executes the corresponding export, and sets up the next trigger.
 */
function _continueExportSequence() {
  const scriptProperties = PropertiesService.getScriptProperties();
  const step = scriptProperties.getProperty('exportStep');

  // Clean up the trigger that just ran this function
  const allTriggers = ScriptApp.getProjectTriggers();
  for (const trigger of allTriggers) {
    if (trigger.getHandlerFunction() === '_continueExportSequence') {
      ScriptApp.deleteTrigger(trigger);
    }
  }

  switch (step) {
    case '1':
      Logger.log('Executing Step 1: Exporting Sub Tasks...');
      exportSubTasksCsvToSheet();
      // Set up the next step
      scriptProperties.setProperty('exportStep', '2');
      ScriptApp.newTrigger('_continueExportSequence').timeBased().after(10 * 1000).create();
      Logger.log('Step 1 complete. Scheduling Step 2 (Escalations).');
      break;

    case '2':
      Logger.log('Executing Step 2: Exporting Escalation Logs...');
      exportEscalationsCsvToSheet();
      // Set up the next step
      scriptProperties.setProperty('exportStep', '3');
      ScriptApp.newTrigger('_continueExportSequence').timeBased().after(10 * 1000).create();
      Logger.log('Step 2 complete. Scheduling Step 3 (Pauses).');
      break;

    case '3':
      Logger.log('Executing Step 3: Exporting Pause Logs...');
      exportPausesCsvToSheet();
      // This is the last step, so we clean up.
      scriptProperties.deleteProperty('exportStep');
      Logger.log('Step 3 complete. Sequential export finished.');
      break;

    default:
      Logger.log('Unknown step or process finished. Cleaning up any remaining properties.');
      scriptProperties.deleteProperty('exportStep');
      break;
  }
}









/// =================================================================
// ===== NEW, ALL-IN-ONE SYNC AND ENRICHMENT FUNCTION (IMPROVED) =====
// =================================================================

/**
 * Performs a safe, comprehensive sync and enrichment for all CSV files.
 * This version performs a TRUE SYNC: it updates existing records if their data
 * has changed in the source, adds missing records, calculates final durations,
 * and creates a detailed audit log of its actions.
 * * To run: Select 'syncAndEnrichAllData' from the function dropdown and click 'Run'.
 */
function syncAndEnrichAllData() {
  Logger.log('Starting comprehensive sync and enrichment process...');

  const subTaskHeaders = SUB_TASK_HEADERS;
  const escalationHeaders = ['Log ID', 'Related Case ID', 'Market', 'Task Type', 'Escalation Start Time', 'Escalation End Time', 'Escalation Reasons', 'OBQ_SLA_Status'];
  const pauseHeaders = ['Log ID', 'Related Case ID', 'Market', 'Task type', 'Pause Start Time', 'Pause End Time'];

  const lock = LockService.getScriptLock();
  lock.waitLock(30000);

  try {
    Logger.log('Fetching and de-duplicating all source records...');
    const deDuplicate = (list1, list2) => {
      const map = new Map();
      list1.forEach(item => { if (item['Log ID']) map.set(String(item['Log ID']), item); });
      list2.forEach(item => { if (item['Log ID']) map.set(String(item['Log ID']), item); });
      return Array.from(map.values());
    };

    const allCases = deDuplicate(fetchFromGoogleSheet_(SOURCE_SUB_TASK_SHEET_ID), fetchFromAppSheetAPI_('Completed_Cases'));
    const allEscalations = deDuplicate(fetchFromGoogleSheet_(SOURCE_ESCALATION_SHEET_ID), fetchFromAppSheetAPI_('Completed_Escalation_Logs'));
    const allPauses = deDuplicate(fetchFromGoogleSheet_(SOURCE_PAUSE_SHEET_ID), fetchFromAppSheetAPI_('Completed_Pause_Logs'));

    Logger.log('Normalizing date formats for frontend compatibility...');
    const dateFieldsToNormalize = ['Asset Trigger Date', 'Asset Delivery Date', 'OnBoarding Done', 'Start Timestamp', 'End Timestamp', 'Case Created'];
    allCases.forEach(task => {
      dateFieldsToNormalize.forEach(field => {
        if (task[field]) {
          const parsedDate = dashboard_assistantParseDateString(task[field]);
          if (parsedDate && !isNaN(parsedDate.getTime())) {
            task[field] = parsedDate.toISOString();
          }
        }
      });
    });

    // --- THIS IS THE FIX: Normalize dates for Escalations and Pauses as well ---
    allEscalations.forEach(log => {
        ['Escalation Start Time', 'Escalation End Time'].forEach(field => {
            if (log[field]) {
                const parsedDate = dashboard_assistantParseDateString(log[field]);
                if(parsedDate && !isNaN(parsedDate.getTime())) {
                    log[field] = parsedDate.toISOString();
                }
            }
        });
    });
    allPauses.forEach(log => {
        ['Pause Start Time', 'Pause End Time'].forEach(field => {
            if (log[field]) {
                const parsedDate = dashboard_assistantParseDateString(log[field]);
                if(parsedDate && !isNaN(parsedDate.getTime())) {
                    log[field] = parsedDate.toISOString();
                }
            }
        });
    });
    // --- END OF FIX ---

    const sourceTasksMap = new Map();
    allCases.forEach(row => { if (row['Log ID']) sourceTasksMap.set(String(row['Log ID']), row); });

    const escalationsByCaseId = new Map();
    allEscalations.forEach(log => {
      if (log['Related Case ID']) {
        if (!escalationsByCaseId.has(log['Related Case ID'])) escalationsByCaseId.set(log['Related Case ID'], []);
        escalationsByCaseId.get(log['Related Case ID']).push(log);
      }
    });
    const pausesByCaseId = new Map();
    allPauses.forEach(log => {
      if (log['Related Case ID']) {
        if (!pausesByCaseId.has(log['Related Case ID'])) pausesByCaseId.set(log['Related Case ID'], []);
        pausesByCaseId.get(log['Related Case ID']).push(log);
      }
    });

    Logger.log('Processing Sub Task CSV...');
    const csvTasks = readCSV(SUB_TASK_FILE_ID);
    const csvTaskIds = new Set(csvTasks.map(row => String(row['Log ID'])));

    const taskUpdateDetails = [];

    csvTasks.forEach(csvRow => {
      const sourceRow = sourceTasksMap.get(String(csvRow['Log ID']));
      if (sourceRow) {
        let changesForThisRow = [];
        const fieldsToSync = ['Date', 'OBQ Reasons', 'Status', 'End Timestamp', 'Comments', 'Escalation Reasons', 'Market', 'Task type', 'User', 'Asset Trigger Date', 'Asset Delivery Date', 'OnBoarding Done'];
        fieldsToSync.forEach(field => {
          const csvValue = String(csvRow[field] || '').trim();
          const sourceValue = String(sourceRow[field] || '').trim();
          if (csvValue !== sourceValue) {
            changesForThisRow.push(`'${field}' from "${csvValue}" to "${sourceValue}"`);
            csvRow[field] = sourceRow[field];
          }
        });
        if (changesForThisRow.length > 0) {
          taskUpdateDetails.push(`Log ID ${csvRow['Log ID']}: ${changesForThisRow.join(', ')}`);
        }
      }
    });

    const newTasks = [];
    sourceTasksMap.forEach((sourceRow, logId) => {
      if (!csvTaskIds.has(logId)) { newTasks.push(sourceRow); }
    });

    const finalTasks = csvTasks.concat(newTasks);
    finalTasks.forEach(task => {
      if (task['Status'] === 'Completed') {
        const relevantEscalations = (escalationsByCaseId.get(task['Case ID']) || []).filter(log => log['Task Type'] === task['Task type']);
        const relevantPauses = (pausesByCaseId.get(task['Case ID']) || []).filter(log => log['Task type'] === task['Task type']);
        task['Stored Escalation Duration'] = calculateTotalDuration_(relevantEscalations, 'Escalation Start Time', 'Escalation End Time');
        task['Stored Paused Duration'] = calculateTotalDuration_(relevantPauses, 'Pause Start Time', 'Pause End Time');
      }
    });

    if (taskUpdateDetails.length > 0) {
      Logger.log(`Updated ${taskUpdateDetails.length} existing tasks.`);
    } else {
      Logger.log('No existing tasks needed updates.');
    }
    Logger.log(`Found ${newTasks.length} new tasks to add.`);

    writeCSV(SUB_TASK_FILE_ID, finalTasks, subTaskHeaders);

    const syncLogs = (fileId, sourceLogs, csvLogs, headers, logName, fieldsToSync) => {
      const sourceLogsMap = new Map(sourceLogs.map(log => [String(log['Log ID']), log]));
      const csvLogIds = new Set(csvLogs.map(row => String(row['Log ID'])));
      const updateDetails = [];

      csvLogs.forEach(csvLog => {
        const sourceLog = sourceLogsMap.get(String(csvLog['Log ID']));
        if (sourceLog) {
          let changes = [];
          fieldsToSync.forEach(field => {
            const csvValue = String(csvLog[field] || '').trim();
            const sourceValue = String(sourceLog[field] || '').trim();
            if(csvValue !== sourceValue){
              changes.push(`'${field}' updated`);
              csvLog[field] = sourceLog[field];
            }
          });
          if(changes.length > 0) updateDetails.push(csvLog['Log ID']);
        }
      });

      const newLogs = sourceLogs.filter(log => log['Log ID'] && !csvLogIds.has(String(log['Log ID'])));

      if(newLogs.length > 0 || updateDetails.length > 0) {
        const combinedLogs = csvLogs.concat(newLogs);
        writeCSV(fileId, combinedLogs, headers);
      }

      if (updateDetails.length > 0) Logger.log(`Updated ${updateDetails.length} existing ${logName} records.`);
      if (newLogs.length > 0) Logger.log(`Found ${newLogs.length} new ${logName} records to add.`);
      if (newLogs.length === 0 && updateDetails.length === 0) Logger.log(`No new or updated ${logName} records found.`);

      return { updatedCount: updateDetails.length, newCount: newLogs.length };
    };

    const csvEscalations = readCSV(ESCALATION_LOGS_FILE_ID);
    const csvPauses = readCSV(PAUSE_LOGS_FILE_ID);

    const escalationSyncResult = syncLogs(ESCALATION_LOGS_FILE_ID, allEscalations, csvEscalations, escalationHeaders, 'Escalation', ['Escalation Start Time', 'Escalation End Time', 'Escalation Reasons']);
    const pauseSyncResult = syncLogs(PAUSE_LOGS_FILE_ID, allPauses, csvPauses, pauseHeaders, 'Pause', ['Pause Start Time', 'Pause End Time']);

    let logDetailsParts = [];
    if (taskUpdateDetails.length > 0) logDetailsParts.push(`Updated ${taskUpdateDetails.length} tasks`);
    if (newTasks.length > 0) logDetailsParts.push(`Added ${newTasks.length} new tasks`);
    if (escalationSyncResult.updatedCount > 0) logDetailsParts.push(`Updated ${escalationSyncResult.updatedCount} escalations`);
    if (escalationSyncResult.newCount > 0) logDetailsParts.push(`Added ${escalationSyncResult.newCount} new escalations`);
    if (pauseSyncResult.updatedCount > 0) logDetailsParts.push(`Updated ${pauseSyncResult.updatedCount} pauses`);
    if (pauseSyncResult.newCount > 0) logDetailsParts.push(`Added ${pauseSyncResult.newCount} new pauses`);

    const logSummary = logDetailsParts.length > 0 ? logDetailsParts.join(', ') + '.' : 'No changes detected.';

    createAuditLog({ user: 'System Sync', action: 'Data Sync', details: logSummary });
    setSystemProperty('lastSync', new Date().toISOString());
    Logger.log('Successfully updated the "lastSync" timestamp and created audit log.');

    Logger.log('Comprehensive sync and enrichment process complete!');
  } catch (e) {
    Logger.log(`A critical error occurred during the sync process: ${e.message}`);
  } finally {
    lock.releaseLock();
  }
}

/**
 * Calculates the total duration in hours from a list of log objects.
 * @private
 */
function calculateTotalDuration_(logs, startKey, endKey) {
  let totalMillis = 0;
  logs.forEach(log => {
    if (log[startKey] && log[endKey]) {
      const start = new Date(log[startKey]);
      const end = new Date(log[endKey]);
      if (end > start) {
        totalMillis += (end - start);
      }
    }
  });
  if (totalMillis === 0) return '00:00:00';
  const totalSeconds = Math.floor(totalMillis / 1000);
  const hours = Math.floor(totalSeconds / 3600);
  const minutes = Math.floor((totalSeconds % 3600) / 60);
  const seconds = totalSeconds % 60;
  const pad = (num) => String(num).padStart(2, '0');
  return `${pad(hours)}:${pad(minutes)}:${pad(seconds)}`;
}




// =================================================================
// ===== NEW ADVANCED DIAGNOSTIC FUNCTION ==========================
// =================================================================

/**
 * A powerful diagnostic tool that compares all data sources and provides a
 * detailed report of duplicates, missing records, and orphaned records.
 * This version has enhanced duplicate detection and reporting.
 * * To run: Select 'runFullDataReconciliationAudit' from the function dropdown and click 'Run'.
 */
function runFullDataReconciliationAudit() {
  Logger.log("--- Starting Full Data Reconciliation Audit ---");
  try {
    // --- Step 1: Fetch data from all sources ---
    Logger.log("Fetching all source data...");
    const activeTasks = fetchFromGoogleSheet_(SOURCE_SUB_TASK_SHEET_ID);
    const activeEscalations = fetchFromGoogleSheet_(SOURCE_ESCALATION_SHEET_ID);
    const activePauses = fetchFromGoogleSheet_(SOURCE_PAUSE_SHEET_ID);

    const completedTasks = fetchFromAppSheetAPI_('Completed_Cases');
    const completedEscalations = fetchFromAppSheetAPI_('Completed_Escalation_Logs');
    const completedPauses = fetchFromAppSheetAPI_('Completed_Pause_Logs');

    const csvTasks = readCSV(SUB_TASK_FILE_ID);
    const csvEscalations = readCSV(ESCALATION_LOGS_FILE_ID);
    const csvPauses = readCSV(PAUSE_LOGS_FILE_ID);

    // --- Step 2: Reconciliation Logic ---
    const reconcile = (name, active, completed, csv, allCompletedTasks) => {
      Logger.log(`\n--- Reconciling: ${name} ---`);

      const activeIds = new Set(active.map(i => String(i['Log ID'])));
      const completedIds = new Set(completed.map(i => String(i['Log ID'])));
      const csvIds = new Set(csv.map(i => String(i['Log ID'])));
      const sourceIds = new Set([...activeIds, ...completedIds]);

      // --- UPGRADED DUPLICATE LOGIC ---
      const completedCaseIds = new Set(allCompletedTasks.map(t => String(t['Case ID'])));
      const overlappingIds = [...activeIds].filter(id => completedIds.has(id));

      const allSourceRecordsMap = new Map();
      active.forEach(rec => allSourceRecordsMap.set(String(rec['Log ID']), rec));
      completed.forEach(rec => allSourceRecordsMap.set(String(rec['Log ID']), rec));

      const trueDuplicates = overlappingIds.filter(id => {
          const record = allSourceRecordsMap.get(id);
          if (!record) return false;
          const caseId = String(record['Related Case ID'] || record['Case ID'] || '');
          // It's only a "true" duplicate if it's an active record whose parent case is NOT completed.
          return !completedCaseIds.has(caseId);
      });

      Logger.log(`Counts: Active=${active.length}, Completed=${completed.length}, Total Unique Source Records=${sourceIds.size}, CSV=${csv.length}`);

      if (trueDuplicates.length > 0) {
        Logger.log(`WARNING: Found ${trueDuplicates.length} TRUE DUPLICATE record(s) between Active and Completed sources (belonging to non-completed cases).`);

        if (name === "Tasks") {
            let adhocProcessCount = 0;
            let adhocAdminCount = 0;
            let normalDuplicates = [];

            trueDuplicates.forEach(id => {
                const record = allSourceRecordsMap.get(id);
                if (record) {
                    const taskType = record['Task type'] || 'Unknown';
                    if (taskType === 'Adhoc-Process') { adhocProcessCount++; }
                    else if (taskType === 'Adhoc-Admin') { adhocAdminCount++; }
                    else {
                        const caseId = record['Case ID'] || 'Unknown';
                        const detailString = `Case ID: ${caseId}, Task Type: ${taskType}`;
                        if (normalDuplicates.length < 20) { normalDuplicates.push(detailString); }
                    }
                }
            });

            const totalNormal = trueDuplicates.length - adhocProcessCount - adhocAdminCount;
            Logger.log(` -> Adhoc-Process Duplicates Found: ${adhocProcessCount}`);
            Logger.log(` -> Adhoc-Admin Duplicates Found: ${adhocAdminCount}`);

            if (totalNormal > 0) {
                Logger.log(` -> Normal Task Duplicates Found: ${totalNormal}`);
                Logger.log(`    (Showing first ${normalDuplicates.length} examples)`);
                normalDuplicates.forEach(detail => Logger.log(`    - ${detail}`));
            }
        } else {
            const duplicateDetails = trueDuplicates.map(id => {
                const record = allSourceRecordsMap.get(id);
                const caseId = record ? (record['Related Case ID'] || 'N/A') : 'N/A';
                return `(Log ID: ${id}, Case ID: ${caseId})`;
            }).slice(0, 50);
            Logger.log(` -> Example Duplicate Details (showing first ${duplicateDetails.length}): ${duplicateDetails.join(', ')}`);
        }
      } else {
        Logger.log("OK: No true duplicates found between Active and Completed sources.");
      }

      // --- NEW: INTERNAL CSV DUPLICATE CHECK ---
      const internalCsvDuplicates = findInternalDuplicates(csv, 'Log ID');
      if (internalCsvDuplicates.length > 0) {
          Logger.log(`CRITICAL WARNING: Found ${internalCsvDuplicates.length} DUPLICATE Log ID(s) within the CSV file itself.`);
          if (name === "Tasks") {
              const duplicateTaskDetails = {};
              internalCsvDuplicates.forEach(logId => {
                  const tasks = csv.filter(t => String(t['Log ID']) === logId);
                  if (tasks.length > 0) {
                      const taskType = tasks[0]['Task type'] || 'Unknown';
                      duplicateTaskDetails[taskType] = (duplicateTaskDetails[taskType] || 0) + 1;
                  }
              });
              Logger.log(" -> Breakdown by Task Type:");
              for (const type in duplicateTaskDetails) {
                  Logger.log(`    - ${type}: ${duplicateTaskDetails[type]} duplicate set(s).`);
              }
          } else {
              Logger.log(` -> Duplicate Log IDs in CSV: ${internalCsvDuplicates.slice(0, 50).join(', ')}`);
          }
      } else {
          Logger.log("OK: No duplicate Log IDs found within the CSV file.");
      }

      const missingFromCsv = [...sourceIds].filter(id => !csvIds.has(id));
      if (missingFromCsv.length > 0) {
        Logger.log(`ERROR: Found ${missingFromCsv.length} record(s) MISSING from the CSV file: ${missingFromCsv.join(', ')}`);
      } else {
        Logger.log("OK: All source records are present in the CSV file.");
      }

      const orphanedInCsv = [...csvIds].filter(id => !sourceIds.has(id));
      if (orphanedInCsv.length > 0) {
        Logger.log(`ERROR: Found ${orphanedInCsv.length} ORPHANED record(s) in the CSV file (deleted from source): ${orphanedInCsv.join(', ')}`);
      } else {
        Logger.log("OK: No orphaned records found in the CSV file.");
      }
    };

    reconcile("Tasks", activeTasks, completedTasks, csvTasks, completedTasks);
    reconcile("Escalations", activeEscalations, completedEscalations, csvEscalations, completedTasks);
    reconcile("Pauses", activePauses, completedPauses, csvPauses, completedTasks);

  } catch(e) {
    Logger.log(`An error occurred during the audit: ${e.message}`);
  }
  Logger.log("\n--- Full Data Reconciliation Audit Complete ---");
}

/**
 * Helper function to find duplicate records within a single array of objects based on a key.
 */
function findInternalDuplicates(data, key) {
    const counts = new Map();
    data.forEach(row => {
        const id = String(row[key] || '');
        if (id) {
            counts.set(id, (counts.get(id) || 0) + 1);
        }
    });
    const duplicates = [];
    counts.forEach((count, id) => {
        if (count > 1) {
            duplicates.push(id);
        }
    });
    return duplicates;
}


// =================================================================
// ===== NEW: ONE-TIME CSV DUPLICATE REMOVAL TOOL ==================
// =================================================================

/**
 * A one-time utility to find and remove duplicate rows from within the CSV files themselves.
 * This function reads each CSV, identifies rows with the same Log ID, keeps only the last
 * instance of each record, and overwrites the file with the clean, de-duplicated data.
 * * To run: Select 'removeInternalCsvDuplicates' from the function dropdown and click 'Run'.
 */
function removeInternalCsvDuplicates() {
  Logger.log("--- Starting Internal CSV Duplicate Removal Process ---");

  const processFile = (fileName, fileId, headers, primaryKey) => {
    Logger.log(`\nProcessing file: ${fileName}...`);
    const data = readCSV(fileId);
    if (data.length === 0) {
      Logger.log(" -> File is empty. No action needed.");
      return;
    }

    const uniqueRecordsMap = new Map();
    data.forEach(row => {
      // The map's key will be the primary key (Log ID). By using .set(), we ensure that
      // if a duplicate key is encountered, it will simply overwrite the
      // previous entry, effectively keeping the LAST instance of that record.
      const key = String(row[primaryKey] || '');
      if (key) {
        uniqueRecordsMap.set(key, row);
      }
    });

    const deDuplicatedData = Array.from(uniqueRecordsMap.values());
    const duplicatesRemoved = data.length - deDuplicatedData.length;

    if (duplicatesRemoved > 0) {
      Logger.log(` -> Found and removed ${duplicatesRemoved} duplicate record(s).`);
      Logger.log(` -> Writing ${deDuplicatedData.length} unique records back to the file.`);
      writeCSV(fileId, deDuplicatedData, headers);
    } else {
      Logger.log(" -> No internal duplicates found in this file.");
    }
  };

  // Define master headers for all files to ensure safe writes
  const escalationHeaders = ['Log ID', 'Related Case ID', 'Market', 'Task Type', 'Escalation Start Time', 'Escalation End Time', 'Escalation Reasons', 'OBQ_SLA_Status'];
  const pauseHeaders = ['Log ID', 'Related Case ID', 'Market', 'Task type', 'Pause Start Time', 'Pause End Time'];

  processFile('Sub Tasks', SUB_TASK_FILE_ID, SUB_TASK_HEADERS, 'Log ID');
  processFile('Escalation Logs', ESCALATION_LOGS_FILE_ID, escalationHeaders, 'Log ID');
  processFile('Pause Logs', PAUSE_LOGS_FILE_ID, pauseHeaders, 'Log ID');

  Logger.log("\n--- Internal CSV Duplicate Removal Process Complete ---");
}


function removeOrphanedRecordsFromCSVs() {
  Logger.log("--- Starting Orphaned Record Removal Process ---");
  const lock = LockService.getScriptLock();
  lock.waitLock(30000);

  try {
    // --- Step 1: Fetch all data from all primary sources to create a master list of valid IDs ---
    Logger.log("Fetching all records from all primary sources to build a master ID list...");
    const activeTasks = fetchFromGoogleSheet_(SOURCE_SUB_TASK_SHEET_ID);
    const completedTasks = fetchFromAppSheetAPI_('Completed_Cases');
    const activeEscalations = fetchFromGoogleSheet_(SOURCE_ESCALATION_SHEET_ID);
    const completedEscalations = fetchFromAppSheetAPI_('Completed_Escalation_Logs');
    const activePauses = fetchFromGoogleSheet_(SOURCE_PAUSE_SHEET_ID);
    const completedPauses = fetchFromAppSheetAPI_('Completed_Pause_Logs');

    const validTaskIds = new Set([...activeTasks.map(t => String(t['Log ID'])), ...completedTasks.map(t => String(t['Log ID']))]);
    const validEscalationIds = new Set([...activeEscalations.map(e => String(e['Log ID'])), ...completedEscalations.map(e => String(e['Log ID']))]);
    const validPauseIds = new Set([...activePauses.map(p => String(p['Log ID'])), ...completedPauses.map(p => String(p['Log ID']))]);

    Logger.log(`Found ${validTaskIds.size} unique source Task IDs.`);
    Logger.log(`Found ${validEscalationIds.size} unique source Escalation Log IDs.`);
    Logger.log(`Found ${validPauseIds.size} unique source Pause Log IDs.`);

    // --- Step 2: Process each CSV file to remove orphans ---
    const processFile = (fileName, fileId, headers, validIdSet, primaryKey) => {
      Logger.log(`\nProcessing file: ${fileName}...`);
      const data = readCSV(fileId);
      if (data.length === 0) {
        Logger.log(" -> File is empty. No action needed.");
        return;
      }

      const cleanedData = data.filter(row => validIdSet.has(String(row[primaryKey])));
      const orphansRemoved = data.length - cleanedData.length;

      if (orphansRemoved > 0) {
        Logger.log(` -> Found and removed ${orphansRemoved} orphaned record(s).`);
        Logger.log(` -> Writing ${cleanedData.length} valid records back to the file.`);
        writeCSV(fileId, cleanedData, headers);
      } else {
        Logger.log(" -> No orphaned records found in this file.");
      }
    };

    // Define master headers for all files to ensure safe writes
    const escalationHeaders = ['Log ID', 'Related Case ID', 'Market', 'Task Type', 'Escalation Start Time', 'Escalation End Time', 'Escalation Reasons', 'OBQ_SLA_Status'];
    const pauseHeaders = ['Log ID', 'Related Case ID', 'Market', 'Task type', 'Pause Start Time', 'Pause End Time'];

    processFile('Sub Tasks', SUB_TASK_FILE_ID, SUB_TASK_HEADERS, validTaskIds, 'Log ID');
    processFile('Escalation Logs', ESCALATION_LOGS_FILE_ID, escalationHeaders, validEscalationIds, 'Log ID');
    processFile('Pause Logs', PAUSE_LOGS_FILE_ID, pauseHeaders, validPauseIds, 'Log ID');

    Logger.log("\n--- Orphaned Record Removal Process Complete ---");

  } catch (e) {
    Logger.log(`A critical error occurred during the orphan removal process: ${e.message}`);
  } finally {
    lock.releaseLock();
  }
}

// =================================================================
// ===== NEW: INTERNAL FULL DATA EXPORT FUNCTION ===================
// =================================================================

/**
 * STARTER FUNCTION: Called by the frontend button.
 * Creates a new spreadsheet, immediately returns its URL, and then schedules
 * the data-filling process to run in the background.
 * @returns {object} An object containing the success status and the new spreadsheet URL.
 */
function startInternalReportExport() {
  Logger.log("--- Starting Internal Full Data Export ---");
  try {
    // ✨ FIX PART 1: Get the email of the person who clicked the button.
    const userEmail = Session.getActiveUser().getEmail();

    // Step 1: Create a new spreadsheet and get its URL.
    const timestamp = Utilities.formatDate(new Date(), Session.getScriptTimeZone(), "yyyy-MM-dd HH:mm");
    const newSpreadsheet = SpreadsheetApp.create(`Internal Full Data Export - ${timestamp}`);
    const reportUrl = newSpreadsheet.getUrl();
    Logger.log(`Report spreadsheet created. URL: ${reportUrl}`);

    // ✨ FIX PART 2: Immediately add the user as an editor without sending an email.
    const newFileId = newSpreadsheet.getId();
    Drive.Permissions.insert({
      'role': 'writer',
      'type': 'user',
      'value': userEmail
    }, newFileId, {
      'sendNotificationEmails': false
    });

    // Step 2: Store the ID of the new sheet so the background process can find it.
    PropertiesService.getScriptProperties().setProperty('tempExportSheetId', newFileId);

    // Step 3: Create a one-time trigger to run the data-filling function in a few seconds.
    ScriptApp.newTrigger('_fillInternalReportSheet')
        .timeBased()
        .after(5 * 1000) // 5 seconds
        .create();

    // Step 4: Immediately return the URL to the frontend.
    return { success: true, reportUrl: reportUrl };

  } catch(e) {
    Logger.log(`A critical error occurred during the internal export initiation: ${e.message}`);
    return { success: false, message: e.message };
  }
}

/**
 * WORKER FUNCTION: Called by a trigger. Do NOT run this manually.
 * This function does the heavy lifting of reading the CSVs and populating the new sheet.
 */
function _fillInternalReportSheet() {
  // Find and delete the trigger that called this function to ensure it only runs once.
  const allTriggers = ScriptApp.getProjectTriggers();
  for (const trigger of allTriggers) {
    if (trigger.getHandlerFunction() === '_fillInternalReportSheet') {
      ScriptApp.deleteTrigger(trigger);
      break;
    }
  }

  const sheetId = PropertiesService.getScriptProperties().getProperty('tempExportSheetId');
  if (!sheetId) {
    Logger.log("Could not find 'tempExportSheetId' property. Aborting fill operation.");
    return;
  }

  try {
    const newSpreadsheet = SpreadsheetApp.openById(sheetId);
    const defaultSheet = newSpreadsheet.getSheets()[0];

    const exportJobs = [
      { sheetName: "Sub Task Data", fileId: SUB_TASK_FILE_ID, headers: SUB_TASK_HEADERS },
      { sheetName: "Escalation Logs", fileId: ESCALATION_LOGS_FILE_ID, headers: ['Log ID', 'Related Case ID', 'Market', 'Task Type', 'Escalation Start Time', 'Escalation End Time', 'Escalation Reasons', 'OBQ_SLA_Status'] },
      { sheetName: "Pause Logs", fileId: PAUSE_LOGS_FILE_ID, headers: ['Log ID', 'Related Case ID', 'Market', 'Task type', 'Pause Start Time', 'Pause End Time'] }
    ];

    exportJobs.forEach(job => {
      const sheet = newSpreadsheet.insertSheet(job.sheetName);
      sheet.getRange(1, 1, 1, job.headers.length).setValues([job.headers]).setFontWeight('bold');
      sheet.setFrozenRows(1);
      const data = readCSV(job.fileId);
      if (data.length > 0) {
        const rowsToWrite = data.map(row => job.headers.map(header => row[header] || ''));
        const batchSize = 500;
        for (let i = 0; i < rowsToWrite.length; i += batchSize) {
          const batch = rowsToWrite.slice(i, i + batchSize);
          sheet.getRange(i + 2, 1, batch.length, job.headers.length).setValues(batch);
          SpreadsheetApp.flush();
        }
      }
    });

    newSpreadsheet.deleteSheet(defaultSheet);
    Logger.log("--- Background data export fill has completed. ---");
  } catch (e) {
    Logger.log(`A critical error occurred during the fill operation: ${e.message}`);
  } finally {
    // Clean up the property to prevent re-use.
    PropertiesService.getScriptProperties().deleteProperty('tempExportSheetId');
  }
}
